{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_items([('net_11', MLP(\n",
       "  (fc1): Linear(in_features=10, out_features=20, bias=True)\n",
       "  (fc2): Linear(in_features=20, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       ")), ('net_seq', Sequential(\n",
       "  (0): Linear(in_features=1, out_features=20, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=20, out_features=1, bias=True)\n",
       ")), ('net_list', ModuleList(\n",
       "  (0): Linear(in_features=1, out_features=20, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=1, out_features=1, bias=True)\n",
       ")), ('out', Linear(in_features=1, out_features=1, bias=True)), ('relu', ReLU())])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class TestMLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim) -> None:\n",
    "        super().__init__()\n",
    "        self.net_11=MLP(input_dim, hidden_dim, output_dim)\n",
    "        self.net_seq=nn.Sequential(\n",
    "            nn.Linear(output_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "        self.net_list=nn.ModuleList([nn.Linear(output_dim, hidden_dim), nn.ReLU(), nn.Linear(output_dim, output_dim)])  \n",
    "        self.out=nn.Linear(output_dim, output_dim)\n",
    "        self.relu=nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x=self.net_11(x)\n",
    "        x=self.net_seq(x)\n",
    "        x=self.net_list(x)\n",
    "        x=self.out(x)\n",
    "        x=self.relu(x)\n",
    "        return x  \n",
    "    \n",
    "mlp=TestMLP(10, 20, 1)\n",
    "mlp._modules.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 8, 8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "test_conv=nn.Conv3d(32, 64, kernel_size=8, stride=1, padding=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_conv.out_channels//test_conv.groups*test_conv.kernel_size[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank=3\n",
    "in_size=test_conv.out_channels//test_conv.groups*test_conv.kernel_size[0]\n",
    "out_size=test_conv.in_channels*test_conv.kernel_size[0]**2\n",
    "lora_weight_B = torch.randn((in_size, rank*test_conv.kernel_size[0]))\n",
    "lora_weight_A = torch.randn((rank*test_conv.kernel_size[0],out_size))\n",
    "lora_bias_B = torch.randn((test_conv.out_channels, rank*test_conv.kernel_size[0]))\n",
    "lora_bias_A = torch.randn((rank*test_conv.kernel_size[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.randn((1, test_conv.in_channels, 32, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(lora_bias_B @ lora_bias_A).squeeze(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21406\n",
      "3324\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3324"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from GIFt.utils import ModuleIterator\n",
    "from GIFt.strategies import FineTuningStrategy,LoRAFullFineTuningStrategy\n",
    "from GIFt.utils import freeze_module,num_trainable_parameters\n",
    "from GIFt.meta_types import FinetuableModule\n",
    "\n",
    "def fine_tuning_sd_hook(module, state_dict, *args, **kwargs):\n",
    "    '''\n",
    "    Clean the state_dict of the module, removing all the parameters that are not trainable.\n",
    "    It is better to remove all the parameters that are not trainable from the state_dict rather than create a new state_dict\n",
    "    rather than create a new state_dict with trainable parameters only. This is because sometimes the state_dict also contains \n",
    "    untrainable buffers, which should be kept in the state_dict.\n",
    "    '''\n",
    "    new_state_dict = {}\n",
    "    not_requires_grad_paras=[name for name,param in module.named_parameters() if not param.requires_grad]\n",
    "    for key, value in state_dict.items():\n",
    "        if key not in not_requires_grad_paras:\n",
    "            new_state_dict[key] = value\n",
    "    return new_state_dict\n",
    "\n",
    "def fine_tuning_loadsd_posthook(module, incompatible_keys):\n",
    "    '''\n",
    "    Enable load_state_dict to load the finetuned model.\n",
    "    The default load_state_dict will raise an error since it also tries to load the unfinetuned parameters.\n",
    "    If you don't want to load this hook, you can also set `strick=False` in `load_state_dict` function.\n",
    "    '''\n",
    "    finetuned_sd_keys=module.state_dict().keys()\n",
    "    key_copys=incompatible_keys.missing_keys.copy()\n",
    "    for key in key_copys:\n",
    "        if key not in finetuned_sd_keys:\n",
    "            incompatible_keys.missing_keys.remove(key)\n",
    "\n",
    "def trainable_parameters(module:nn.Module,recurse:bool=True):\n",
    "    for name, param in module.named_parameters(recurse=recurse):\n",
    "        if param.requires_grad:\n",
    "            yield param\n",
    "\n",
    "def num_trainable_parameters(module:nn.Module):\n",
    "    return sum(p.numel() for p in trainable_parameters(module))\n",
    "\n",
    "def num_parameters(module:nn.Module):\n",
    "    return sum(p.numel() for p in module.parameters())\n",
    "\n",
    "def replace_modules(module:nn.Module,fine_tuning_strategy:FineTuningStrategy,parent_name:str=\"\"):\n",
    "    # Replace layers with finetuable layers\n",
    "    for name, global_name, class_name, layer_obj, has_child in ModuleIterator(module,parent_name):\n",
    "        find=False\n",
    "        if isinstance(layer_obj,FinetuableModule):\n",
    "            raise ValueError(f\"Layer {global_name} is already finetuable\")\n",
    "        for check_func,act_func in fine_tuning_strategy:\n",
    "            if check_func(name, global_name, class_name, layer_obj):\n",
    "                act_func(module,name, global_name, class_name, layer_obj)\n",
    "                find=True\n",
    "                break\n",
    "        if not find and has_child:\n",
    "            replace_modules(layer_obj,fine_tuning_strategy,name)\n",
    "        else:\n",
    "            freeze_module(layer_obj)\n",
    "\n",
    "def enable_fine_tuning(module:nn.Module,fine_tuning_strategy:FineTuningStrategy):\n",
    "    # replace modules\n",
    "    replace_modules(module,fine_tuning_strategy)\n",
    "    # add hook to the module to remove untrainable parameters from the state_dict\n",
    "    module._register_state_dict_hook(fine_tuning_sd_hook)\n",
    "    # add hook to the module to enable load_state_dict to load the finetuned model\n",
    "    module.register_load_state_dict_post_hook(fine_tuning_loadsd_posthook)\n",
    "    # add trainable_parameters function to the module\n",
    "    setattr(module,\"trainable_parameters\",lambda recurse=True: trainable_parameters(module,recurse))\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "mlp=TestMLP(100, 200, 1)\n",
    "#print(mlp.state_dict().keys())\n",
    "print(num_trainable_parameters(mlp))\n",
    "lora_strategy=LoRAFullFineTuningStrategy()\n",
    "enable_fine_tuning(mlp,lora_strategy)\n",
    "print(num_trainable_parameters(mlp))\n",
    "#print(mlp.state_dict().keys())\n",
    "current_sd=mlp.state_dict()\n",
    "mlp.load_state_dict(current_sd)\n",
    "mlp.trainable_parameters()\n",
    "optimizer=torch.optim.Adam(mlp.trainable_parameters(),lr=0.01)\n",
    "mlp.parameters()\n",
    "mlp.trainable_parameters()\n",
    "num_trainable_parameters(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8,)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_conv=nn.Conv1d(32, 64, kernel_size=8, stride=1, padding=1)\n",
    "test_conv.kernel_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.functional.dropout(torch.randn(10),p=0.5,training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1921e-07,  2.9802e-07, -2.3842e-07,  0.0000e+00, -5.9605e-08],\n",
       "        [-1.1921e-07, -3.5763e-07, -2.3842e-07,  0.0000e+00,  7.1526e-07],\n",
       "        [ 1.1921e-07,  0.0000e+00,  0.0000e+00, -8.1956e-08,  0.0000e+00],\n",
       "        [-5.9605e-08, -2.3842e-07,  1.1921e-07, -3.5763e-07,  4.7684e-07],\n",
       "        [-7.1526e-07,  0.0000e+00, -4.7684e-07,  9.5367e-07,  1.7881e-07],\n",
       "        [ 4.7684e-07, -1.7881e-07, -3.5763e-07,  4.7684e-07, -4.7684e-07],\n",
       "        [ 0.0000e+00,  0.0000e+00, -2.3842e-07, -1.1921e-07, -2.3842e-07],\n",
       "        [ 1.1921e-07, -1.1921e-07,  0.0000e+00,  0.0000e+00,  4.7684e-07],\n",
       "        [-4.7684e-07, -2.3842e-07,  1.1921e-07,  4.7684e-07,  2.3842e-07],\n",
       "        [ 2.9802e-07,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.1921e-07],\n",
       "        [-5.9605e-08, -2.3842e-07,  2.3842e-07, -2.3842e-07, -1.1921e-07],\n",
       "        [ 2.3842e-07, -1.7881e-07, -1.1921e-07, -5.9605e-08,  2.3842e-07],\n",
       "        [-4.7684e-07,  2.3842e-07, -2.3842e-07,  4.7684e-07,  0.0000e+00],\n",
       "        [-2.9802e-08, -2.3842e-07, -2.3842e-07,  2.3842e-07, -1.7881e-07],\n",
       "        [ 8.9407e-08, -2.3842e-07, -4.7684e-07,  2.3842e-07, -4.7684e-07],\n",
       "        [-4.7684e-07,  1.1921e-07, -3.7253e-08, -2.3842e-07,  0.0000e+00],\n",
       "        [-2.3842e-07, -1.1921e-07,  0.0000e+00,  0.0000e+00,  2.3842e-07],\n",
       "        [ 5.9605e-08,  0.0000e+00,  1.1921e-07, -1.1921e-07,  4.7684e-07],\n",
       "        [-4.7684e-07, -5.9605e-08,  2.3842e-07,  2.3842e-07,  0.0000e+00],\n",
       "        [ 1.7509e-07,  1.1921e-07,  2.3842e-07,  4.7684e-07,  5.9605e-08]],\n",
       "       grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear=nn.Linear(10,5)\n",
    "x=torch.randn((20,10))\n",
    "weight=torch.randn_like(linear.weight)\n",
    "bias=torch.zeros_like(linear.bias)\n",
    "nn.functional.linear(x,weight+linear.weight,bias+linear.bias)-linear(x)-nn.functional.linear(x,weight,bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 10])\n",
      "torch.Size([5, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear=nn.Linear(10,5)\n",
    "x=torch.randn((20,10))\n",
    "print(x.shape)\n",
    "print(linear.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear=nn.Linear(10,5).to(\"cuda\")\n",
    "x=torch.randn((20,10)).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100000):\n",
    "    linear(x)+((linear.weight @ x.T).T + linear.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias=torch.zeros_like(linear.bias)\n",
    "for i in range(100000):\n",
    "    linear(x)+torch.functional.F.linear(x,linear.weight,bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100000):\n",
    "    torch.functional.F.linear(x,linear.weight+linear.weight,linear.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "10\n",
      "[1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "class myclass():\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        self.ml=[1,2,3,4,5]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ml)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.ml[index]\n",
    "\n",
    "a=myclass()\n",
    "for a_i in a:\n",
    "    print(a_i)\n",
    "\n",
    "def add_func(self,x):\n",
    "    print(x)\n",
    "    print(self.ml)\n",
    "\n",
    "setattr(a, \"additional\", lambda x: add_func(a,x))\n",
    "a.additional(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter\n",
      "iter\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "21\n",
      "iter\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "class MyNumbers:\n",
    "  def __iter__(self):\n",
    "    print(\"iter\")\n",
    "    self.a = 1\n",
    "    return self\n",
    "\n",
    "  def __next__(self):\n",
    "    if self.a <= 20:\n",
    "      x = self.a\n",
    "      self.a += 1\n",
    "      return x\n",
    "    else:\n",
    "      raise StopIteration\n",
    "\n",
    "myclass = MyNumbers()\n",
    "myiter = iter(myclass)\n",
    "\n",
    "for x in myiter:\n",
    "  print(myiter.a)\n",
    "print(myiter.a)  \n",
    "for x in myiter:\n",
    "  print(myiter.a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

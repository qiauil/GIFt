{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_items([('net_11', MLP(\n",
       "  (fc1): Linear(in_features=10, out_features=20, bias=True)\n",
       "  (fc2): Linear(in_features=20, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       ")), ('net_seq', Sequential(\n",
       "  (0): Linear(in_features=1, out_features=20, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=20, out_features=1, bias=True)\n",
       ")), ('net_list', ModuleList(\n",
       "  (0): Linear(in_features=1, out_features=20, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=1, out_features=1, bias=True)\n",
       ")), ('out', Linear(in_features=1, out_features=1, bias=True)), ('relu', ReLU())])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class TestMLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim) -> None:\n",
    "        super().__init__()\n",
    "        self.net_11=MLP(input_dim, hidden_dim, output_dim)\n",
    "        self.net_seq=nn.Sequential(\n",
    "            nn.Linear(output_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "        self.net_list=nn.ModuleList([nn.Linear(output_dim, hidden_dim), nn.ReLU(), nn.Linear(output_dim, output_dim)])  \n",
    "        self.out=nn.Linear(output_dim, output_dim)\n",
    "        self.relu=nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x=self.net_11(x)\n",
    "        x=self.net_seq(x)\n",
    "        x=self.net_list(x)\n",
    "        x=self.out(x)\n",
    "        x=self.relu(x)\n",
    "        return x  \n",
    "    \n",
    "mlp=TestMLP(10, 20, 1)\n",
    "mlp._modules.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21406\n",
      "3324\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3324"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from GIFt.utils import ModuleIterator\n",
    "from GIFt.strategies import FineTuningStrategy,LoRAFullFineTuningStrategy\n",
    "from GIFt.utils import freeze_module,num_trainable_parameters\n",
    "from GIFt.meta_types import FinetuableModule\n",
    "\n",
    "def finetuning_sd_hook(module, state_dict, *args, **kwargs):\n",
    "    '''\n",
    "    Clean the state_dict of the module, removing all the parameters that are not trainable.\n",
    "    It is better to remove all the parameters that are not trainable from the state_dict rather than create a new state_dict\n",
    "    rather than create a new state_dict with trainable parameters only. This is because sometimes the state_dict also contains \n",
    "    untrainable buffers, which should be kept in the state_dict.\n",
    "    '''\n",
    "    new_state_dict = {}\n",
    "    not_requires_grad_paras=[name for name,param in module.named_parameters() if not param.requires_grad]\n",
    "    for key, value in state_dict.items():\n",
    "        if key not in not_requires_grad_paras:\n",
    "            new_state_dict[key] = value\n",
    "    return new_state_dict\n",
    "\n",
    "def finetuning_loadsd_posthook(module, incompatible_keys):\n",
    "    '''\n",
    "    Enable load_state_dict to load the finetuned model.\n",
    "    The default load_state_dict will raise an error since it also tries to load the unfinetuned parameters.\n",
    "    If you don't want to load this hook, you can also set `strick=False` in `load_state_dict` function.\n",
    "    '''\n",
    "    finetuned_sd_keys=module.state_dict().keys()\n",
    "    key_copys=incompatible_keys.missing_keys.copy()\n",
    "    for key in key_copys:\n",
    "        if key not in finetuned_sd_keys:\n",
    "            incompatible_keys.missing_keys.remove(key)\n",
    "\n",
    "def trainable_parameters(module:nn.Module,recurse:bool=True):\n",
    "    for name, param in module.named_parameters(recurse=recurse):\n",
    "        if param.requires_grad:\n",
    "            yield param\n",
    "\n",
    "def num_trainable_parameters(module:nn.Module):\n",
    "    return sum(p.numel() for p in trainable_parameters(module))\n",
    "\n",
    "def num_parameters(module:nn.Module):\n",
    "    return sum(p.numel() for p in module.parameters())\n",
    "\n",
    "def replace_modules(module:nn.Module,finetuning_strategy:FineTuningStrategy,parent_name:str=\"\"):\n",
    "    # Replace layers with finetuable layers\n",
    "    for name, global_name, class_name, layer_obj, has_child in ModuleIterator(module,parent_name):\n",
    "        find=False\n",
    "        if isinstance(layer_obj,FinetuableModule):\n",
    "            raise ValueError(f\"Layer {global_name} is already finetuable\")\n",
    "        for check_func,act_func in finetuning_strategy:\n",
    "            if check_func(name, global_name, class_name, layer_obj):\n",
    "                act_func(module,name, global_name, class_name, layer_obj)\n",
    "                find=True\n",
    "                break\n",
    "        if not find and has_child:\n",
    "            replace_modules(layer_obj,finetuning_strategy,name)\n",
    "        else:\n",
    "            freeze_module(layer_obj)\n",
    "\n",
    "def enable_finetuning(module:nn.Module,finetuning_strategy:FineTuningStrategy):\n",
    "    # replace modules\n",
    "    replace_modules(module,finetuning_strategy)\n",
    "    # add hook to the module to remove untrainable parameters from the state_dict\n",
    "    module._register_state_dict_hook(finetuning_sd_hook)\n",
    "    # add hook to the module to enable load_state_dict to load the finetuned model\n",
    "    module.register_load_state_dict_post_hook(finetuning_loadsd_posthook)\n",
    "    # add trainable_parameters function to the module\n",
    "    setattr(module,\"trainable_parameters\",lambda recurse=True: trainable_parameters(module,recurse))\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "mlp=TestMLP(100, 200, 1)\n",
    "#print(mlp.state_dict().keys())\n",
    "print(num_trainable_parameters(mlp))\n",
    "lora_strategy=LoRAFullFineTuningStrategy()\n",
    "enable_finetuning(mlp,lora_strategy)\n",
    "print(num_trainable_parameters(mlp))\n",
    "#print(mlp.state_dict().keys())\n",
    "current_sd=mlp.state_dict()\n",
    "mlp.load_state_dict(current_sd)\n",
    "mlp.trainable_parameters()\n",
    "optimizer=torch.optim.Adam(mlp.trainable_parameters(),lr=0.01)\n",
    "mlp.parameters()\n",
    "mlp.trainable_parameters()\n",
    "num_trainable_parameters(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "10\n",
      "[1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "class myclass():\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        self.ml=[1,2,3,4,5]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ml)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.ml[index]\n",
    "\n",
    "a=myclass()\n",
    "for a_i in a:\n",
    "    print(a_i)\n",
    "\n",
    "def add_func(self,x):\n",
    "    print(x)\n",
    "    print(self.ml)\n",
    "\n",
    "setattr(a, \"additional\", lambda x: add_func(a,x))\n",
    "a.additional(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter\n",
      "iter\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "21\n",
      "iter\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "class MyNumbers:\n",
    "  def __iter__(self):\n",
    "    print(\"iter\")\n",
    "    self.a = 1\n",
    "    return self\n",
    "\n",
    "  def __next__(self):\n",
    "    if self.a <= 20:\n",
    "      x = self.a\n",
    "      self.a += 1\n",
    "      return x\n",
    "    else:\n",
    "      raise StopIteration\n",
    "\n",
    "myclass = MyNumbers()\n",
    "myiter = iter(myclass)\n",
    "\n",
    "for x in myiter:\n",
    "  print(myiter.a)\n",
    "print(myiter.a)  \n",
    "for x in myiter:\n",
    "  print(myiter.a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=10, out_features=100, bias=True)\n",
      "    (1-4): 4 x Linear(in_features=100, out_features=100, bias=True)\n",
      "    (5): Linear(in_features=100, out_features=5, bias=True)\n",
      "  )\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Network before enable finetuning: MLP(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=10, out_features=100, bias=True)\n",
      "    (1-4): 4 x Linear(in_features=100, out_features=100, bias=True)\n",
      "    (5): Linear(in_features=100, out_features=5, bias=True)\n",
      "  )\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Number of trainable parameters: 42005\n",
      "Network afer enable finetuning: MLP(\n",
      "  (layers): ModuleList(\n",
      "    (0): LoRALinear(\n",
      "      (parent_module): Linear(in_features=10, out_features=100, bias=True)\n",
      "    )\n",
      "    (1-4): 4 x LoRALinear(\n",
      "      (parent_module): Linear(in_features=100, out_features=100, bias=True)\n",
      "    )\n",
      "    (5): LoRALinear(\n",
      "      (parent_module): Linear(in_features=100, out_features=5, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Number of trainable parameters after fine-tuning: 3045\n",
      "dict_keys(['layers.0.lora_B', 'layers.0.lora_A', 'layers.1.lora_B', 'layers.1.lora_A', 'layers.2.lora_B', 'layers.2.lora_A', 'layers.3.lora_B', 'layers.3.lora_A', 'layers.4.lora_B', 'layers.4.lora_A', 'layers.5.lora_B', 'layers.5.lora_A'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from GIFt import enable_finetuning\n",
    "from GIFt.strategies import LoRAFullFineTuningStrategy\n",
    "from GIFt.utils import num_trainable_parameters\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim,num_layers):\n",
    "        super(MLP, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "        for i in range(num_layers-1):\n",
    "            self.layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "        self.layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.layers[i](x)\n",
    "            x = self.relu(x)\n",
    "        x = self.layers[-1](x)\n",
    "        return x\n",
    "\n",
    "mlp=MLP(10, 100, 5, 5)\n",
    "print(mlp)\n",
    "print(\"Network before enable finetuning:\",mlp)\n",
    "print(\"Number of trainable parameters:\",num_trainable_parameters(mlp))\n",
    "# Enable fine-tuning with specified strategy\n",
    "# LoRAFullFineTuningStrategy() will replace all Linear layers with LoRA layers and all Conv1/2/3D layers with LoRAConv1/2/3D layers\n",
    "enable_finetuning(mlp, LoRAFullFineTuningStrategy())\n",
    "print(\"Network afer enable finetuning:\",mlp)\n",
    "print(\"Number of trainable parameters after fine-tuning:\",num_trainable_parameters(mlp))\n",
    "# use `trainable_parameters()` rather than `parameters()` to get the trainable parameters for fine-tuning\n",
    "# Also avaliable through `GIFt.utils.trainable_parameters()`\n",
    "optimizer = torch.optim.Adam(mlp.trainable_parameters(), lr=0.001)\n",
    "# After using `enable_finetuning`, the state dict of the model will be updated to only include the trainable parameters\n",
    "state_dict = mlp.state_dict()\n",
    "print(state_dict.keys())\n",
    "torch.save(state_dict, \"mlp.pth\")\n",
    "mlp.load_state_dict(torch.load(\"mlp.pth\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

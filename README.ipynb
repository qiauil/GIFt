{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GIFt: Generic and Intuitive Fine-tuning Library\n",
    "\n",
    "## Examples of using GIFt for fine-tuning:\n",
    "\n",
    "First, Let's build a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=100, bias=True)\n",
      "    (1-4): 4 x Linear(in_features=100, out_features=100, bias=True)\n",
      "    (5): Linear(in_features=100, out_features=1, bias=True)\n",
      "  )\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Network before enable finetuning: MLP(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=100, bias=True)\n",
      "    (1-4): 4 x Linear(in_features=100, out_features=100, bias=True)\n",
      "    (5): Linear(in_features=100, out_features=1, bias=True)\n",
      "  )\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Number of trainable parameters: 40701\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from GIFt import enable_finetuning\n",
    "from GIFt.strategies import LoRAFullFineTuningStrategy\n",
    "from GIFt.utils import num_trainable_parameters\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim,num_layers):\n",
    "        super(MLP, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "        for i in range(num_layers-1):\n",
    "            self.layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "        self.layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.layers[i](x)\n",
    "            x = self.relu(x)\n",
    "        x = self.layers[-1](x)\n",
    "        return x\n",
    "\n",
    "mlp=MLP(1, 100, 1, 5)\n",
    "print(mlp)\n",
    "print(\"Network before enable fine-tuning:\",mlp)\n",
    "print(\"Number of trainable parameters:\",num_trainable_parameters(mlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can enable fine-tuning for this neural network with a single line of command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network afer enable finetuning: MLP(\n",
      "  (layers): ModuleList(\n",
      "    (0): LoRALinear(\n",
      "      (parent_module): Linear(in_features=1, out_features=100, bias=True)\n",
      "    )\n",
      "    (1-4): 4 x LoRALinear(\n",
      "      (parent_module): Linear(in_features=100, out_features=100, bias=True)\n",
      "    )\n",
      "    (5): LoRALinear(\n",
      "      (parent_module): Linear(in_features=100, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Number of trainable parameters after fine-tuning: 3006\n"
     ]
    }
   ],
   "source": [
    "enable_finetuning(mlp, LoRAFullFineTuningStrategy())\n",
    "print(\"Network after enable fine-tuning:\",mlp)\n",
    "print(\"Number of trainable parameters after fine-tuning:\",num_trainable_parameters(mlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `LoRAFullFineTuningStrategy` is a subclass of `FineTuningStrategy` where it can replace all Linear layers with LoRA Linear layers and all Conv1/2/3D layers with LoRAConv1/2/3D layers. We will discuss how to build up a new fine-tuning strategy later.\n",
    "\n",
    "After fine-tuning with the `enable_finetuning` function, there will be a new function, `trainable_parameters()`, of the network instance. We can use this function for the setup of the optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(mlp.trainable_parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use `GIFt.utils.trainable_parameters(mlp)` to get the trainable parameters.\n",
    "\n",
    "Besides, after enabling fine-tuning for the neural network, the statedict of the model will be updated to only include the trainable parameters (fine-tuned parameters). Thus, we can directly save and load the weights of the fine-tuned network as the conventional way we usually do in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['layers.0.lora_B', 'layers.0.lora_A', 'layers.1.lora_B', 'layers.1.lora_A', 'layers.2.lora_B', 'layers.2.lora_A', 'layers.3.lora_B', 'layers.3.lora_A', 'layers.4.lora_B', 'layers.4.lora_A', 'layers.5.lora_B', 'layers.5.lora_A'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = mlp.state_dict()\n",
    "print(state_dict.keys())\n",
    "torch.save(state_dict, \"mlp.pth\")\n",
    "mlp.load_state_dict(torch.load(\"mlp.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training strategy\n",
    "\n",
    "`enable_finetuning` function requires an instance of `FineTuningStrategy` class. Actually, any iterable object returns a `check` function, and an `action` function works for the `enable_finetuning` function. Here, the `check` function checks whether the layer satisfies some specific condition, and the `action` function will be activated if the `check` function returns true.\n",
    "\n",
    "The parameters of the `check` and `action` functions are `name, global_name, class_name, layer_obj` and `module, name, global_name, class_name, layer_obj` respectively. Let's use a simple example to show how you a fine-tuning strategy and the meaning of these parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example MLP: ExampleMLP(\n",
      "  (in_model): Linear(in_features=1, out_features=10, bias=True)\n",
      "  (mid_model): MLP(\n",
      "    (layers): ModuleList(\n",
      "      (0-2): 3 x Linear(in_features=10, out_features=10, bias=True)\n",
      "    )\n",
      "    (relu): ReLU()\n",
      "  )\n",
      "  (out_model): Linear(in_features=10, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class ExampleMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ExampleMLP, self).__init__()\n",
    "        self.in_model=nn.Linear(1, 10)\n",
    "        self.mid_model=MLP(10, 10, 10, 2)\n",
    "        self.out_model=nn.Linear(10, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "example_mlp=ExampleMLP()\n",
    "print(\"Example MLP:\",example_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following strategy will replace all the linear layer with convolution layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module ExampleMLP(\n",
      "  (in_model): Linear(in_features=1, out_features=10, bias=True)\n",
      "  (mid_model): MLP(\n",
      "    (layers): ModuleList(\n",
      "      (0-2): 3 x Linear(in_features=10, out_features=10, bias=True)\n",
      "    )\n",
      "    (relu): ReLU()\n",
      "  )\n",
      "  (out_model): Linear(in_features=10, out_features=1, bias=True)\n",
      ")\n",
      "Layer name: in_model\n",
      "Global name: in_model\n",
      "Class name: Linear\n",
      "Layer object: Linear(in_features=1, out_features=10, bias=True)\n",
      "__________________________________________________\n",
      "Module ModuleList(\n",
      "  (0-2): 3 x Linear(in_features=10, out_features=10, bias=True)\n",
      ")\n",
      "Layer name: 0\n",
      "Global name: layers.0\n",
      "Class name: Linear\n",
      "Layer object: Linear(in_features=10, out_features=10, bias=True)\n",
      "__________________________________________________\n",
      "Module ModuleList(\n",
      "  (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (1-2): 2 x Linear(in_features=10, out_features=10, bias=True)\n",
      ")\n",
      "Layer name: 1\n",
      "Global name: layers.1\n",
      "Class name: Linear\n",
      "Layer object: Linear(in_features=10, out_features=10, bias=True)\n",
      "__________________________________________________\n",
      "Module ModuleList(\n",
      "  (0-1): 2 x Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (2): Linear(in_features=10, out_features=10, bias=True)\n",
      ")\n",
      "Layer name: 2\n",
      "Global name: layers.2\n",
      "Class name: Linear\n",
      "Layer object: Linear(in_features=10, out_features=10, bias=True)\n",
      "__________________________________________________\n",
      "Module ExampleMLP(\n",
      "  (in_model): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mid_model): MLP(\n",
      "    (layers): ModuleList(\n",
      "      (0-2): 3 x Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "    )\n",
      "    (relu): ReLU()\n",
      "  )\n",
      "  (out_model): Linear(in_features=10, out_features=1, bias=True)\n",
      ")\n",
      "Layer name: out_model\n",
      "Global name: out_model\n",
      "Class name: Linear\n",
      "Layer object: Linear(in_features=10, out_features=1, bias=True)\n",
      "__________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class ExampleStrategy():\n",
    "    def __init__(self):\n",
    "        \n",
    "        def check_function(name, global_name, class_name, layer_obj):\n",
    "            if class_name == \"Linear\":\n",
    "                return True\n",
    "            return False\n",
    "        \n",
    "        def action_function(module,name, global_name, class_name, layer_obj):\n",
    "            print(\"Module\",module)\n",
    "            print(\"Layer name:\",name)\n",
    "            print(\"Global name:\",global_name)\n",
    "            print(\"Class name:\",class_name)\n",
    "            print(\"Layer object:\",layer_obj)\n",
    "            print(\"_\"*50)\n",
    "            setattr(module, name, nn.Conv2d(\n",
    "                layer_obj.in_features, layer_obj.out_features,\n",
    "                kernel_size=3\n",
    "            ))\n",
    "        self.check_actions=[(check_function, action_function)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.check_actions)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.check_actions[index]\n",
    "    \n",
    "enable_finetuning(example_mlp, ExampleStrategy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Form the previous example, we can know that:\n",
    "* `enable_finetuning` function iterates over all layers from top to bottom, from outside to inside.\n",
    "* `module` parameter is a `nn.Module` representing the parent module of current layer.\n",
    "* `layer_name` parameter is a `str` representing the name of current layer.\n",
    "* `global_name` parameter is a `str` representing the global name of current layer, i.e., it contains all the name of parent layers.\n",
    "* `layer_obj` is a `nn.Module` representing the current layer.\n",
    "\n",
    "`FineTuningStrategy` class is a helper class which makes your procedure of designing the training strategy more simpler. It also support additional parameters for the action `function`. You can refer to the source code of `LoRAFullFineTuningStrategy()` to see how it works. Here we give an example of using `FineTuningStrategy` class to build up the previous strategy: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExampleMLP(\n",
      "  (in_model): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mid_model): MLP(\n",
      "    (layers): ModuleList(\n",
      "      (0-2): 3 x Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "    )\n",
      "    (relu): ReLU()\n",
      "  )\n",
      "  (out_model): Conv2d(10, 1, kernel_size=(3, 3), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from typing import Callable, Dict, Sequence, Tuple\n",
    "from GIFt.strategies import FineTuningStrategy\n",
    "import GIFt.utils.factories as fts\n",
    "\n",
    "class ExampleStrategy2(FineTuningStrategy):\n",
    "    \n",
    "    def __init__(self, kernel_size=3) -> None:\n",
    "        default_action_paras = {\"conv_para\":{\"kernel_size\": 3}}\n",
    "        customized_action_paras = {\"conv_para\":{\"kernel_size\": kernel_size}}\n",
    "        checks_actions_parnames = [\n",
    "            (fts.c_cname_func(\"Linear\"),\n",
    "             fts.a_replace_func(lambda layer_obj,kernel_size: nn.Conv2d(layer_obj.in_features, layer_obj.out_features, kernel_size=kernel_size)),\n",
    "             \"conv_para\")\n",
    "        ]\n",
    "        super().__init__(checks_actions_parnames, default_action_paras, customized_action_paras)\n",
    "\n",
    "example_mlp=ExampleMLP()\n",
    "enable_finetuning(example_mlp, ExampleStrategy2())\n",
    "print(example_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good thing of using `FineTuningStrategy` is that we can extract the parameters of fine-tuning and save them separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'conv_para': {'kernel_size': 3}}\n",
      "{'lora_paras': {'rank': 3, 'lora_alpha': None, 'lora_dropout': 0.0, 'train_bias': False}}\n"
     ]
    }
   ],
   "source": [
    "print(ExampleStrategy2().paras())\n",
    "print(LoRAFullFineTuningStrategy().paras())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing we need to mention is that we recommend making all the new layers in the fine-tuning model an instance of `GIFt.meta_types.FinetuableModule` as the `enable_finetuning` function will check whether a layer is already an instance of the `FinetuableModule` to avoid incorrectly duplicate setting networks.\n",
    "\n",
    "## An example of applying LoRA fine-tuning to attention layers\n",
    "\n",
    "The `Q`, `K`, and `V` matrix in attention layer can be calculated through `Linear` layer if the input is a sequence or `Conv` layer is the input is a field. Thus, we can simply use our previous code to enable LoRA fine-tuning for attention layers:\n",
    "\n",
    "Build up attention layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from einops import rearrange\n",
    "\n",
    "# more examples of attention implementation can be found in my another repository:\n",
    "# https://github.com/qiauil/Foxutils/blob/main/foxutils/network/attentions.py\n",
    "class MultiHeadAttentionBase(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_heads:int,linear_attention=False, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        queries,keys,values =map(self.apart_input,(queries,keys,values))\n",
    "        d_k = keys.shape[-1]\n",
    "        weights = torch.bmm(queries, keys.transpose(1,2)) / math.sqrt(d_k)\n",
    "        weights = nn.functional.softmax(weights, dim=-1)\n",
    "        return self.concat_output(torch.bmm(self.dropout(weights), values))\n",
    "\n",
    "    def apart_input(self,x):\n",
    "        #(batch_size, num_elements, num_heads$\\times$dim_deads)  >>> (batch_size, num_elements, num_heads, dim_deads)\n",
    "        x = x.reshape(x.shape[0], x.shape[1], self.num_heads, -1)\n",
    "        #(batch_size, num_elements, num_heads, dim_deads) >>> (batch_size, num_heads, num_elements, dim_deads) \n",
    "        x = x.permute(0, 2, 1, 3)\n",
    "        #(batch_size, num_heads, num_elements, dim_deads)  >>> (batch_size$\\times$num_heads, num_elements, dim_deads) \n",
    "        return x.reshape(-1, x.shape[2], x.shape[3])\n",
    "\n",
    "\n",
    "    def concat_output(self, x):\n",
    "        #(batch_size$\\times$num_heads, num_elements, dim_deads) >>> (batch_size, num_heads, num_elements, dim_deads)\n",
    "        x = x.reshape(-1, self.num_heads, x.shape[1], x.shape[2])\n",
    "        #(batch_size, num_heads, num_elements, dim_deads) >>> (batch_size, num_elements, num_heads, dim_deads)\n",
    "        x = x.permute(0, 2, 1, 3)\n",
    "        #(batch_size, num_elements, num_heads, dim_deads) >>> (batch_size, num_elements, num_heads$\\times$dim_deads)\n",
    "        return x.reshape(x.shape[0], x.shape[1], -1)\n",
    "\n",
    "class SequenceMultiHeadAttention(nn.Module):\n",
    "    def __init__(self,dim_q:int, dim_k:int, dim_v:int, num_heads:int, dim_heads:int,dim_out:int, linear_attention=False, dropout=0.0,bias=False):\n",
    "        super().__init__()\n",
    "        dim_hiddens=num_heads*dim_heads\n",
    "        self.w_q = nn.Linear(dim_q, dim_hiddens,bias=bias)\n",
    "        self.w_k = nn.Linear(dim_k, dim_hiddens,bias=bias)\n",
    "        self.w_v = nn.Linear(dim_v, dim_hiddens,bias=bias)\n",
    "        self.mha=MultiHeadAttentionBase(num_heads=num_heads,linear_attention=linear_attention,dropout=dropout)\n",
    "        self.w_o = nn.Linear(dim_hiddens, dim_out,bias=bias)\n",
    "    \n",
    "    def forward(self, queries, keys, values):\n",
    "        q=self.w_q(queries)\n",
    "        k=self.w_k(keys)\n",
    "        v=self.w_v(values)\n",
    "        att=self.mha(q,k,v)\n",
    "        return self.w_o(att)\n",
    "\n",
    "class TwoDFieldMultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self,dim_q, dim_k, dim_v, num_heads, dim_heads,dim_out, linear_attention=False, dropout=0.0,bias=False):\n",
    "        super().__init__()\n",
    "        dim_hiddens=num_heads*dim_heads\n",
    "        self.w_q = nn.Conv2d(dim_q, dim_hiddens, 1, bias=bias)\n",
    "        self.w_k = nn.Conv2d(dim_k, dim_hiddens, 1, bias=bias)\n",
    "        self.w_v = nn.Conv2d(dim_v, dim_hiddens, 1, bias=bias)\n",
    "        self.mha=MultiHeadAttentionBase(num_heads=num_heads,linear_attention=linear_attention,dropout=dropout)\n",
    "        self.w_o = nn.Conv2d(dim_hiddens, dim_out,1,bias=bias)\n",
    "    \n",
    "    def forward(self, queries, keys, values):\n",
    "        width=queries.shape[-1]\n",
    "        q=self.w_q(queries)\n",
    "        k=self.w_k(keys)\n",
    "        v=self.w_v(values)\n",
    "        q, k, v = map(lambda t: rearrange(t, \"b c h w -> b (h w) c\"), (q,k,v))\n",
    "        att=self.mha(q,k,v)\n",
    "        att_2D=rearrange(att,\"b (h w) c -> b c h w\",w=width)\n",
    "        return self.w_o(att_2D)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_attention before enable finetuning:\n",
      "SequenceMultiHeadAttention(\n",
      "  (w_q): Linear(in_features=10, out_features=10, bias=False)\n",
      "  (w_k): Linear(in_features=10, out_features=10, bias=False)\n",
      "  (w_v): Linear(in_features=10, out_features=10, bias=False)\n",
      "  (mha): MultiHeadAttentionBase()\n",
      "  (w_o): Linear(in_features=10, out_features=10, bias=False)\n",
      ")\n",
      "sequence_attention after enable finetuning:\n",
      "SequenceMultiHeadAttention(\n",
      "  (w_q): LoRALinear(\n",
      "    (parent_module): Linear(in_features=10, out_features=10, bias=False)\n",
      "  )\n",
      "  (w_k): LoRALinear(\n",
      "    (parent_module): Linear(in_features=10, out_features=10, bias=False)\n",
      "  )\n",
      "  (w_v): LoRALinear(\n",
      "    (parent_module): Linear(in_features=10, out_features=10, bias=False)\n",
      "  )\n",
      "  (mha): MultiHeadAttentionBase()\n",
      "  (w_o): LoRALinear(\n",
      "    (parent_module): Linear(in_features=10, out_features=10, bias=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "field attention before enable finetuning:\n",
      "field attention after enable finetuning:\n",
      "TwoDFieldMultiHeadAttention(\n",
      "  (w_q): Conv2d(10, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (w_k): Conv2d(10, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (w_v): Conv2d(10, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (mha): MultiHeadAttentionBase()\n",
      "  (w_o): Conv2d(10, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      ")\n",
      "TwoDFieldMultiHeadAttention(\n",
      "  (w_q): LoRAConv2d(\n",
      "    (parent_module): Conv2d(10, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  )\n",
      "  (w_k): LoRAConv2d(\n",
      "    (parent_module): Conv2d(10, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  )\n",
      "  (w_v): LoRAConv2d(\n",
      "    (parent_module): Conv2d(10, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  )\n",
      "  (mha): MultiHeadAttentionBase()\n",
      "  (w_o): LoRAConv2d(\n",
      "    (parent_module): Conv2d(10, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "sequence_attention=SequenceMultiHeadAttention(10,10,10,2,5,10)\n",
    "print(\"sequence_attention before enable finetuning:\")\n",
    "print(sequence_attention)\n",
    "enable_finetuning(sequence_attention, LoRAFullFineTuningStrategy())\n",
    "print(\"sequence_attention after enable finetuning:\")\n",
    "print(sequence_attention)\n",
    "print(\"\")\n",
    "print(\"field attention before enable finetuning:\")\n",
    "field_attention=TwoDFieldMultiHeadAttention(10,10,10,2,5,10)\n",
    "print(\"field attention after enable finetuning:\")\n",
    "print(field_attention)\n",
    "enable_finetuning(field_attention, LoRAFullFineTuningStrategy())\n",
    "print(field_attention)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

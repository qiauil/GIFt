{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GIFt: Generic and Intuitive Fine-tuning Library\n",
    "\n",
    "Fine-tuning is a common technique in deep learning to adapt a pre-trained model to a new task. It is widely used in computer vision, natural language processing, and other domains. However, fine-tuning is not always straightforward. It requires a good understanding of the model, the dataset, and the task. In this notebook, we introduce GIFt, a generic and intuitive fine-tuning library that simplifies the process of fine-tuning pre-trained models. GIFt is designed to be easy to use, flexible, and extensible. \n",
    "\n",
    "## Quick Start\n",
    "\n",
    "### Structure of a neural network and Caps\n",
    "A modern neural network usually consists of many layers. In each layer, there are two main components: a submodule which links to the next layer (or not) and some independent parameters (at the last layer, there will only be parameters).\n",
    "\n",
    "```\n",
    "Network\n",
    "- module1\n",
    "    - submodule1\n",
    "        - parameters\n",
    "        - subsubmodule1\n",
    "            - ***\n",
    "                ***\n",
    "                -lastsubmodule\n",
    "                    - parameter\n",
    "    - submodule2\n",
    "        - parameters\n",
    "- parameters\n",
    "```\n",
    "\n",
    "The core of fine-tuning a network is to modify part of the submodule and parameters and freeze the rest with the pre-trained result. To enable this, we introduce the concept of `Caps`, i.e., \"check-action-parameters\". A `Caps` is a sequence of tuple where each tuple contains three elements: a check function, an action function, and a parameter. The check function is used to determine whether the submodule or parameter should be modified. The action function is used to modify the submodule or parameter. The parameter is the value that will be used in the action function. With a designed `Caps`, we can easily fine-tune a network.\n",
    "\n",
    "The allowed check function in `GIFt` should have the following structure:\n",
    "\n",
    "for submodule:\n",
    "```\n",
    "check_func (function):\n",
    "    A function that takes in the following parameters and returns True if the module meets the condition, False otherwise.\n",
    "    \n",
    "    - parent_module (nn.Module): The parent module.\n",
    "    - current_name (str): The name of current module.\n",
    "    - global_name (str): The global name of current modul.\n",
    "    - class_name (str): The class name of current module.\n",
    "    - current_module (nn.Module): The current module object.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the module meets the condition, False otherwise.\n",
    "```\n",
    "\n",
    "for parameter:\n",
    "```\n",
    "check_func: \n",
    "    - A function that takes in the following parameters and returns True if the parameter meets the condition, False otherwise.\n",
    "    \n",
    "    - parent_module (nn.Module): The parent module.\n",
    "    - current_name (str): The name of current module.\n",
    "    - global_name (str): The global name of current modul.\n",
    "    - class_name (str): The class name of current module.\n",
    "    - current_module (nn.Module): The current module object.\n",
    "    - parameter_name (str): The name of current parameter.\n",
    "    - parameter (nn.Parameter): The current parameter object. \n",
    "    \n",
    "    Returns:\n",
    "        bool: True if the parameter meets the condition, False otherwise.\n",
    "```\n",
    "\n",
    "The corresponding action function receives the same parameters as the check function and modifies the submodule or parameter. You can also set additional parameters to the action function with the `parameter` in caps tuple.\n",
    "\n",
    "All the above parameters can be get through the `ModuleIterator` class in `GIFt`. Here is an simple example\n",
    ":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1 TestNet.conv1 Conv2d False\n",
      "> TestNet.conv1 weight\n",
      "> TestNet.conv1 bias\n",
      "pool TestNet.pool MaxPool2d False\n",
      "fc1 TestNet.fc1 Linear False\n",
      "> TestNet.fc1 weight\n",
      "> TestNet.fc1 bias\n",
      "nn_seq TestNet.nn_seq Sequential True\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from GIFt import ModuleIterator\n",
    "\n",
    "class TestNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.nn_seq = nn.Sequential(\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, 10)\n",
    "        )\n",
    "\n",
    "net=TestNet()\n",
    "iterator=ModuleIterator(net,\"TestNet\")\n",
    "for current_name, global_name, class_name, current_module, has_children in iterator:\n",
    "    print(current_name, global_name, class_name, has_children)\n",
    "    for para_name, param in current_module.named_parameters(recurse=False):\n",
    "        print(\">\", global_name, para_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `GIFt.factories` module, we provide some common check functions and action functions. You can also define your own check functions and action functions. \n",
    "\n",
    "### Finetuning strategy\n",
    "\n",
    "In `GIFt`, we use a `FineTuningStrategy` class to orgainze the `Caps`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTuningStrategy(InitParaRecorder):\n",
    "    \"\"\"\n",
    "    A class representing a fine-tuning strategy.\n",
    "\n",
    "    Args:\n",
    "        module_caps (Optional[Sequence[Tuple[Callable[[nn.Module,str,str,str,nn.Module],bool], Callable, dict]], optional):\n",
    "            A list of tuples containing the check function, action function, and action parameters for modules.\n",
    "            Defaults to an empty list.\n",
    "        para_caps (Optional[Sequence[Tuple[Callable[[nn.Module,str,str,str,nn.Module,str,nn.Parameter],bool], Callable, dict]], optional):\n",
    "            A list of tuples containing the check function, action function, and action parameters for parameters.\n",
    "            Defaults to an empty list.\n",
    "        constraint_type (Optional[Union[Sequence[Type], Type]], optional): \n",
    "            A list of types or a single type. In `enable_fine_tuning`, the module type will be checked against this list.\n",
    "            The strategy will only be applied to modules of the specified type(s).\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`FineTuningStrategy` collects all the `Caps` and can be applied to a network. You may notice that there is an additional initialization parameter `constraint_type` in the class. You can set a specific type to this `constraint_type` to make sure this strategy only works on the specific type of module. Note that this type check is not working inside of `FineTuningStrategy` but in the `enable_fine_tuning` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enable_fine_tuning(module:nn.Module,\n",
    "                      fine_tuning_strategy:FineTuningStrategy,\n",
    "                      replace_parameter_function:bool=True):\n",
    "    \"\"\"\n",
    "    Enable fine-tuning for a given module.\n",
    "\n",
    "    Args:\n",
    "        module (nn.Module): The module to enable fine-tuning for.\n",
    "        fine_tuning_strategy (FineTuningStrategy): The strategy to use for fine-tuning.\n",
    "        replace_parameter_function (bool): Whether to replace the `parameters` function of the module.\n",
    "            If True, the `parameters` function will only return trainable parameters. This helps you \n",
    "            avoiding you modifying your optimizer initialization code. If you set it as False, you \n",
    "            can use the `trainable_parameters` function from `GIFt.utils.network_tool` to get trainable parameters of \n",
    "            your network for an optimizer.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`enable_fine_tuning` function is the main function to apply the `FineTuningStrategy` to a network. It will iterate through all the modules and parameters in the network and apply the `Caps` to the network. It will also replace the state_dict of the network to make sure that everytime you save or load the model, you will only save or load the fine-tuned part.\n",
    "\n",
    "In `enable_fine_tuning` function, It will first run the check function for modules, if check function returns True, it will run the action function (If you just don't want to make any changes to the module but just keep it training, your action function can just do nothing and return None). Then it will run the check function for parameters, if check function returns True, it will run the action function, otherwise, it will freeze the parameter. Finally, If it detects that current module has a submodule, it will recursively run the `enable_fine_tuning` function on the submodule.\n",
    "\n",
    "Currently, we provide some bilit-in `FineTuningStrategy` in `GIFt.strategies` module. You can also define your own `FineTuningStrategy` by inheriting the `FineTuningStrategy` class. Here is a very simple example to enable fine-tuning on all the `nn.Linear` modules with LoRA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before fine-tuning, the number of trainable parameters is: 11689512\n",
      "After fine-tuning, the number of trainable parameters is: 75063\n",
      "--------------------------------------------------\n",
      "index | Name                         | Type      | Shape\n",
      "--------------------------------------------------\n",
      "0     | conv1.lora_B                 | [448, 3]  | 1344 \n",
      "1     | conv1.lora_A                 | [3, 21]   | 63   \n",
      "2     | layer1.0.conv1.lora_B        | [192, 3]  | 576  \n",
      "3     | layer1.0.conv1.lora_A        | [3, 192]  | 576  \n",
      "4     | layer1.0.conv2.lora_B        | [192, 3]  | 576  \n",
      "5     | layer1.0.conv2.lora_A        | [3, 192]  | 576  \n",
      "6     | layer1.1.conv1.lora_B        | [192, 3]  | 576  \n",
      "7     | layer1.1.conv1.lora_A        | [3, 192]  | 576  \n",
      "8     | layer1.1.conv2.lora_B        | [192, 3]  | 576  \n",
      "9     | layer1.1.conv2.lora_A        | [3, 192]  | 576  \n",
      "10    | layer2.0.conv1.lora_B        | [384, 3]  | 1152 \n",
      "11    | layer2.0.conv1.lora_A        | [3, 192]  | 576  \n",
      "12    | layer2.0.conv2.lora_B        | [384, 3]  | 1152 \n",
      "13    | layer2.0.conv2.lora_A        | [3, 384]  | 1152 \n",
      "14    | layer2.0.downsample.0.lora_B | [128, 3]  | 384  \n",
      "15    | layer2.0.downsample.0.lora_A | [3, 64]   | 192  \n",
      "16    | layer2.1.conv1.lora_B        | [384, 3]  | 1152 \n",
      "17    | layer2.1.conv1.lora_A        | [3, 384]  | 1152 \n",
      "18    | layer2.1.conv2.lora_B        | [384, 3]  | 1152 \n",
      "19    | layer2.1.conv2.lora_A        | [3, 384]  | 1152 \n",
      "20    | layer3.0.conv1.lora_B        | [768, 3]  | 2304 \n",
      "21    | layer3.0.conv1.lora_A        | [3, 384]  | 1152 \n",
      "22    | layer3.0.conv2.lora_B        | [768, 3]  | 2304 \n",
      "23    | layer3.0.conv2.lora_A        | [3, 768]  | 2304 \n",
      "24    | layer3.0.downsample.0.lora_B | [256, 3]  | 768  \n",
      "25    | layer3.0.downsample.0.lora_A | [3, 128]  | 384  \n",
      "26    | layer3.1.conv1.lora_B        | [768, 3]  | 2304 \n",
      "27    | layer3.1.conv1.lora_A        | [3, 768]  | 2304 \n",
      "28    | layer3.1.conv2.lora_B        | [768, 3]  | 2304 \n",
      "29    | layer3.1.conv2.lora_A        | [3, 768]  | 2304 \n",
      "30    | layer4.0.conv1.lora_B        | [1536, 3] | 4608 \n",
      "31    | layer4.0.conv1.lora_A        | [3, 768]  | 2304 \n",
      "32    | layer4.0.conv2.lora_B        | [1536, 3] | 4608 \n",
      "33    | layer4.0.conv2.lora_A        | [3, 1536] | 4608 \n",
      "34    | layer4.0.downsample.0.lora_B | [512, 3]  | 1536 \n",
      "35    | layer4.0.downsample.0.lora_A | [3, 256]  | 768  \n",
      "36    | layer4.1.conv1.lora_B        | [1536, 3] | 4608 \n",
      "37    | layer4.1.conv1.lora_A        | [3, 1536] | 4608 \n",
      "38    | layer4.1.conv2.lora_B        | [1536, 3] | 4608 \n",
      "39    | layer4.1.conv2.lora_A        | [3, 1536] | 4608 \n",
      "40    | fc.lora_B                    | [1000, 3] | 3000 \n",
      "41    | fc.lora_A                    | [3, 512]  | 1536 \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models.resnet import resnet18\n",
    "from GIFt import enable_fine_tuning\n",
    "from GIFt.strategies.lora import LoRAAllFineTuningStrategy\n",
    "from GIFt.utils.info import collect_trainable_parameters,table_info\n",
    "\n",
    "net=resnet18()\n",
    "paras_info,num_paras=collect_trainable_parameters(net)\n",
    "print(\"Before fine-tuning, the number of trainable parameters is:\",num_paras)\n",
    "enable_fine_tuning(net,LoRAAllFineTuningStrategy())\n",
    "paras_info,num_paras=collect_trainable_parameters(net)\n",
    "print(\"After fine-tuning, the number of trainable parameters is:\",num_paras)\n",
    "print(table_info(paras_info,header=[\"index\",\"Name\",\"Type\",\"Shape\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
